{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xuruoxin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/xuruoxin/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xuruoxin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/xuruoxin/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Imports \n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('covid2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some basic cleaning of the data\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates(subset='text')\n",
    "# drop the rows that user_followers =0\n",
    "df = df[df['user_followers'] != 0]\n",
    "# drop NA values\n",
    "df = df.dropna()\n",
    "# reset index\n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text column and do text cleaning(most common methods are covered)\n",
    "def preprocess_text(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    # remove numbers\n",
    "    text = re.sub(r\"\\d\", \"\", text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Removing extra spaces\n",
    "    text = \" \".join(tokens)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    # Removing Emojis\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Removing emoticons\n",
    "    text = re.sub(r':\\w+:', '', text)\n",
    "    \n",
    "    # Removing Contractions\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(column):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', column)\n",
    "\n",
    "df[\"user_name\"] = df[\"user_name\"].apply(remove_emojis)\n",
    "df[\"user_location\"] = df[\"user_location\"].apply(remove_emojis)\n",
    "df[\"user_description\"] = df[\"user_description\"].apply(remove_emojis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the user_location column\n",
    "df['user_location'] = df['user_location'].str.replace('[^\\w\\s]','')\n",
    "df = df[df['user_location'].notna()]\n",
    "text = \" \".join(df['user_location'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background\n",
    "Problem statement: Retail industries, such as epidemic prevention supplies companies, need to consider the future impact of COVID-19 to develop appropriate marketing strategies and risk management plans. \n",
    "\n",
    "Real world impact: Retail industries need to be prepared to adjust their business models and sales strategies quickly. Covid-19 has forced consumers to rely more heavily on online shopping, which means retailers need to further strengthen their website capabilities, such as virtual fitting rooms and online payment options. Besides, retailers need to strengthen their supply chain to ensure timely delivery and adequate inventory.\n",
    "By providing retailers with our semantic clustering model for covid-19, retailers can make preliminary predictions about consumers’ buying habits and spending patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive widget\n",
    "def unique_sorted_values(array):\n",
    "    unique = array.unique().tolist()\n",
    "    unique.sort()\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location = ['United States', 'United Kingdom','India','Canada','Switzerland','China']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa87fede26a473fa9eeb6916e80d7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('Canada', 'China', 'India', 'Switzerland', 'United Kingdom', 'United States'), value='Canada…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5decefd1aa147fd8d320dae4b0c0b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dropdown_country = widgets.Dropdown(options = unique_sorted_values(pd.Series(df_location)))\n",
    "output_country = widgets.Output()\n",
    "\n",
    "def dropdown_country_eventhandler(change):\n",
    "    output_country.clear_output()\n",
    "    with output_country:\n",
    "        country_df = df.loc[df.user_location == change.new]\n",
    "# sentiment analysis using the VADER lexicon\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        country_df['sentiment_score'] = country_df['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "        country_df['sentiment'] = country_df['sentiment_score'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))\n",
    "\n",
    "# Plot the count of each sentiment\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.countplot(country_df['sentiment'], order=['Positive', 'Neutral', 'Negative'])\n",
    "        plt.xlabel(\"Sentiment\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(\"Count of Sentiments in the Tweet dataset\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "dropdown_country.observe(dropdown_country_eventhandler, names = 'value')\n",
    "display(dropdown_country)\n",
    "\n",
    "display(output_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retailers can choose to understand the sentiment analysis of different countries. By selecting a specific country, retailers can know whether the country's overall attitude towards covid-19 is positive, negative or neural. This would help them to decide which country may have a high demand for their products or which country probably can be their raw material processing location, so as to seize business opportunities and develop product import and export trade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1cbcd2a69c4699a7e41ad11827fc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('Canada', 'China', 'India', 'Switzerland', 'United Kingdom', 'United States'), value='Canada…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d4ca2410a443ea80bb476c27ff755b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dropdown_country1 = widgets.Dropdown(options = unique_sorted_values(pd.Series(df_location)))\n",
    "output_country1 = widgets.Output()\n",
    "\n",
    "def dropdown_country_eventhandler1(change):\n",
    "    output_country1.clear_output()\n",
    "    with output_country1:\n",
    "        \n",
    "\n",
    "# Remove stop words and calculate the frequency of the words\n",
    "        country_df = df.loc[df.user_location== change.new]\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        all_words = ' '.join(country_df['text'].tolist())\n",
    "        words = nltk.word_tokenize(all_words)\n",
    "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "        fdist = nltk.FreqDist(words)\n",
    "\n",
    "# Plot the word cloud of the most common words\n",
    "        wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(fdist)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "dropdown_country1.observe(dropdown_country_eventhandler1, names = 'value')\n",
    "display(dropdown_country1)\n",
    "\n",
    "display(output_country1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By clicking different countries, retailers can obtain a word cloud that contains the most frequent words of covid tweets for that country. Through keywords generated by our model, retailers can analyze consumers’ life status, such as financial situation, emotional needs, and demand for epidemic prevention supplies. It is helpful for them to adjust product prices, advertising content, and inventory accordingly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
